{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The COCO object recognition model is loaded on the JetBot and an array of object indexes is created based on the image captured by the camera. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import ObjectDetector\n",
    "\n",
    "\n",
    "model = ObjectDetector('../ssd_mobilenet_v2_coco.engine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the camera and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 1, 'confidence': 0.9349912405014038, 'bbox': [0.34503233432769775, 0.6683716773986816, 0.7439225912094116, 0.9909040927886963]}, {'label': 76, 'confidence': 0.9102267026901245, 'bbox': [0.009082138538360596, 0.6157286763191223, 0.3107420802116394, 1.0]}, {'label': 72, 'confidence': 0.5443169474601746, 'bbox': [0.0008737891912460327, 0.0, 0.4643605947494507, 0.5210582613945007]}, {'label': 1, 'confidence': 0.4122934937477112, 'bbox': [0.31259989738464355, 0.3512469530105591, 1.0, 0.9818780422210693]}]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.system('systemctl restart nvargus-daemon')\n",
    "\n",
    "from jetbot import Camera\n",
    "\n",
    "camera = Camera.instance(width=300, height=300)\n",
    "\n",
    "detections = model(camera.value)\n",
    "\n",
    "print(detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a widget to display detected objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42dc3aa2c5d64c44b82e42f8259b83c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value=\"[[{'label': 1, 'confidence': 0.82332843542099, 'bbox': [0.007861822843551636, 0.35590872168540…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets.widgets as widgets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "detections_widget = widgets.Textarea()\n",
    "\n",
    "detections_widget.value = str(detections)\n",
    "\n",
    "display(detections_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detections: The “label” value indicates which object has been detected, the “confidence” value is the probability that the recognized object is the actual object, and the “bbox” values are the bounding box parameters of the object in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 1, 'confidence': 0.82332843542099, 'bbox': [0.007861822843551636, 0.35590872168540955, 0.3116206228733063, 0.654403805732727]}\n"
     ]
    }
   ],
   "source": [
    "if detections:\n",
    "    image_number = 0\n",
    "    object_number = 0\n",
    "    print(detections[image_number][object_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the best_path training model for collision avoidance. Preprocess camera values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "collision_model = torchvision.models.alexnet(pretrained=False)\n",
    "collision_model.classifier[6] = torch.nn.Linear(collision_model.classifier[6].in_features, 2)\n",
    "collision_model.load_state_dict(torch.load('best_model_FreeBlocked.pth'))\n",
    "device = torch.device('cuda')\n",
    "collision_model = collision_model.to(device)\n",
    "\n",
    "mean = 255.0 * np.array([0.485, 0.456, 0.406])\n",
    "stdev = 255.0 * np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "normalize = torchvision.transforms.Normalize(mean, stdev)\n",
    "\n",
    "def preprocess(camera_value):\n",
    "    global device, normalize\n",
    "    x = camera_value\n",
    "    x = cv2.resize(x, (224, 224))\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = x.transpose((2, 0, 1))\n",
    "    x = torch.from_numpy(x).float()\n",
    "    x = normalize(x)\n",
    "    x = x.to(device)\n",
    "    x = x[None, ...]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the JetBot controls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the driving instructions based on object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c79828f4a24f2c99629b19a0be00d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from jetbot import Robot\n",
    "robot = Robot()\n",
    "from jetbot import bgr8_to_jpeg\n",
    "from IPython.display import display\n",
    "import ipywidgets.widgets as widgets\n",
    "import traitlets\n",
    "blocked_widget = widgets.FloatSlider(min=0.0, max=1.0, value=0.0, description='blocked')\n",
    "#image_widget = widgets.Image(format='jpeg', width=300, height=300)\n",
    "label_widget = widgets.IntText(value=1, description='tracked label')\n",
    "\n",
    "image_widget = widgets.Image(format='jpeg', width=224, height=224)  # this width and height doesn't necessarily have to match the camera\n",
    "\n",
    "camera_link = traitlets.dlink((camera, 'value'), (image_widget, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "robot.stop()\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([image_widget, blocked_widget]),\n",
    "    label_widget\n",
    "]))\n",
    "\n",
    "width = int(image_widget.width)\n",
    "height = int(image_widget.height)\n",
    "\n",
    "def detection_center(detection):\n",
    "    \"\"\"Computes the center x, y coordinates of the object\"\"\"\n",
    "    bbox = detection['bbox']\n",
    "    center_x = (bbox[0] + bbox[2]) / 2.0 - 0.5\n",
    "    center_y = (bbox[1] + bbox[3]) / 2.0 - 0.5\n",
    "    return (center_x, center_y)\n",
    "    \n",
    "def norm(vec):\n",
    "    \"\"\"Computes the length of the 2D vector\"\"\"\n",
    "    return np.sqrt(vec[0]**2 + vec[1]**2)\n",
    "\n",
    "def closest_detection(detections):\n",
    "    \"\"\"Finds the detection closest to the image center\"\"\"\n",
    "    closest_detection = None\n",
    "    for det in detections:\n",
    "        center = detection_center(det)\n",
    "        if closest_detection is None:\n",
    "            closest_detection = det\n",
    "        elif norm(detection_center(det)) < norm(detection_center(closest_detection)):\n",
    "            closest_detection = det\n",
    "    return closest_detection\n",
    "\n",
    "# Simple PD controller (Kp - proportional term, Kd - derivative term)\n",
    "\n",
    "Kp = 0.18\n",
    "Kd = 0.05\n",
    "\n",
    "def update(change):\n",
    "    global robot, frame_counter, n_frames_stuck\n",
    "    x = change['new'] \n",
    "    x = preprocess(x)\n",
    "    y = model(x)\n",
    "    \n",
    "    # apply the `softmax` function to normalize the output vector so it sums to 1 (which makes it a probability distribution)\n",
    "    y = F.softmax(y, dim=1)\n",
    "    \n",
    "    y = y.flatten()\n",
    "          \n",
    "def execute(change):\n",
    "    image = change['new']\n",
    "    \n",
    "    # execute collision model to determine if blocked\n",
    "    collision_output = collision_model(preprocess(image)).detach().cpu()\n",
    "    prob_blocked = float(F.softmax(collision_output.flatten(), dim=0)[0]) # probability of blocked, first dataset category\n",
    "    prob_caution = float(F.softmax(collision_output.flatten(), dim=0)[1]) # probability of caution, second dataset category\n",
    "    prob_free = float(F.softmax(collision_output.flatten(), dim=0)[2]) # probability of free, third dataset category\n",
    "    blocked_widget.value = prob_blocked\n",
    "    \n",
    "    min_drive = .5\n",
    "    min_blocked = .6\n",
    "    min_turn = .5                         # Turn slow\n",
    "    max_turn = .7                         # Turn fast\n",
    "    frwd_value = 0.6                      # Default value to drive forward (0 = no action, 1 = full motor capacity)\n",
    "    frwd_caution = 0.4                    # Caution speed\n",
    "    bkwd_value = 0.5                      # Default reverse value\n",
    "    min_free = .40    # Min free for robot to drive forward \n",
    "    max_n_frames_stuck = 10               # Limit on the number of frames the robot is stuck for. Once this limit is reached, robot goes in reverse\n",
    "    frame_counter = 0                     # Frame counter \n",
    "    n_frames_stuck = 0                    # Initialize counter of the number of successive frames the robot is stuck for  \n",
    "    data_log = []                         # Initialize the array whcih will store a history of telemetry readings and robot actions (for analysis and tuning) \n",
    "    \n",
    "    # define functions which determine (and return) robot actions\n",
    "    def forward(value):\n",
    "        robot.forward(value)\n",
    "        return (\"FWRD\",round(value,2))\n",
    "\n",
    "    def left(value):\n",
    "        robot.left(value)\n",
    "        return (\"LEFT\",round(value,2))\n",
    "\n",
    "    def right(value):\n",
    "        robot.right(value)\n",
    "        return (\"RIGHT\",round(value,2))\n",
    "    \n",
    "    def reverse(value):\n",
    "        robot.backward(value)\n",
    "        return (\"BKWRD\",round(value,2))\n",
    "    \n",
    "    action = \"\"\n",
    "            \n",
    "    # compute all detected objects\n",
    "    detections = model(image)\n",
    "\n",
    "    # draw a box around detection on image\n",
    "    for det in detections[0]:\n",
    "        bbox = det['bbox']\n",
    "        cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (255, 0, 0), 2)\n",
    "        \n",
    "\n",
    "    # select detections that match selected class label\n",
    "    matching_detections = [d for d in detections[0] if d['label'] == int(label_widget.value)]\n",
    "    # get detection closest to center of field of view and draw it\n",
    "    det = closest_detection(matching_detections) \n",
    "    \n",
    "    detectionList = []\n",
    "    \n",
    "    if det: # true if there are any detections\n",
    "            bbox = det['bbox']\n",
    "            cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (0, 255, 0), 5)          \n",
    "            \n",
    "            # move robot forward and steer proportional target's x-distance from center\n",
    "            center = detection_center(det)\n",
    "            robot.set_motors(\n",
    "                float(frwd_value + min_turn * center[0]),\n",
    "                float(frwd_value - min_turn * center[0])\n",
    "            )\n",
    "            image_widget.value = bgr8_to_jpeg(image)\n",
    "            detectionList = detectionList.append(det)\n",
    "    if prob_free < prob_caution or prob_blocked:\n",
    "        if prob_caution < prob_blocked:\n",
    "            if n_frames_stuck < max_n_frames_stuck: \n",
    "                action = right(max_turn)\n",
    "                n_frames_stuck = n_frames_stuck + 1\n",
    "                image_widget.value = bgr8_to_jpeg(image)\n",
    "                return\n",
    "            else:\n",
    "                action = reverse(bkwd_value)\n",
    "                n_frames_stuck = 0\n",
    "\n",
    "        else:\n",
    "            action = right(min_turn)\n",
    "            action = frwd_caution\n",
    "            n_frames_stuck = n_frames_stuck + 1\n",
    "            return\n",
    "    else:\n",
    "        robot.forward(frwd_value)\n",
    "\n",
    "execute({'new': camera.value}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect the update function to each camera update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.unobserve_all()\n",
    "camera.observe(execute, names='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End processing and stop the robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.unobserve_all()\n",
    "time.sleep(1.0)\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}