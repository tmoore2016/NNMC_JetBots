{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Road Following Data Collection - PLUS v2.0\n",
    "Author: George Gorospe\n",
    "\n",
    "Note: this code draws heavily on the NVIDIA data_collection.ipynb notebook shipped with the Jetbot sold by Sparkfun.com.\n",
    "You can find the original code here: https://github.com/NVIDIA-AI-IOT/jetbot\n",
    "\n",
    "Here, our goal is to enable our Jetbot to follow a road designated by a lane or limits on the left and right of the road. This is similar to line following. However, unlike line following, no lines are made for the robot, the Jetbot will now have to adapt to its environment.\n",
    "\n",
    "Essentially you'll be teaching your Jetbot how to drive while staying inside the limits of the road or path. To do this, lets consider what type of data we need to collect...\n",
    "\n",
    "We want our Jetbot to know how to steer given its position on the road or track.\n",
    "So we'll need many photos and steering angles guiding the robot to a safe trajectory or path at various points in the road.\n",
    "\n",
    "In this notebook, we are going to control the Jetbot, commanding it to move along the road, while we collect data (photos) and viable path data (x and y coordinates). That can be later used to train a machine learning model. In this case, we're training the model to steer for us. Let's get started!"
   ]
  },
  {
   "source": [
    "Road Following\n",
    "If you've run through the collision avoidance sample, your should be familiar following three steps\n",
    "\n",
    "Data collection\n",
    "Training\n",
    "Deployment\n",
    "In this notebook, we'll do the same exact thing! Except, instead of classification, you'll learn a different fundamental technique, regression, that we'll use to enable JetBot to follow a road (or really, any path or target point).\n",
    "\n",
    "Place the JetBot in different positions on a path (offset from center, different angles, etc)\n",
    "Remember from collision avoidance, data variation is key!\n",
    "\n",
    "Display the live camera feed from the robot\n",
    "Using a gamepad controller, place a 'green dot', which corresponds to the target direction we want the robot to travel, on the image.\n",
    "Store the X, Y values of this green dot along with the image from the robot's camera\n",
    "Then, in the training notebook, we'll train a neural network to predict the X, Y values of our label. In the live demo, we'll use the predicted X, Y values to compute an approximate steering value (it's not 'exactly' an angle, as that would require image calibration, but it's roughly proportional to the angle so our controller will work fine).\n",
    "\n",
    "So how do you decide exactly where to place the target for this example? Here is a guide we think may help\n",
    "\n",
    "Assuming our deep learning model works as intended, these labeling guidelines should ensure the following:\n",
    "\n",
    "The robot can safely travel directly towards the target (without going out of bounds etc.)\n",
    "The target will continuously progress along our imagined path\n",
    "What we get, is a 'carrot on a stick' that moves along our desired trajectory. Deep learning decides where to place the carrot, and JetBot just follows it :)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Labeling example video\n",
    "Execute the block of code to see an example of how to we labeled the images. This model worked after only 123 images :)\n",
    "\n",
    "Look at the live video feed from the camera\n",
    "Imagine the path that the robot should follow (try to approximate the distance it needs to avoid running off road etc.)\n",
    "Place the target as far along this path as it can go so that the robot could head straight to the target without 'running off' the road.\n",
    "For example, if we're on a very straight road, we could place it at the horizon. If we're on a sharp turn, it may need to be placed closer to the robot so it doesn't run out of boundaries."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/FW4En6LejhI\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following python libraries are required for the data collection process.\n",
    "If you get an error here, you probably need to install the clickable image widget.\n",
    "Follow the instructions here: https://github.com/jaybdub/jupyter_clickable_image_widget\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IPython Libraries for display and widgets\n",
    "import traitlets\n",
    "import ipywidgets\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "from jupyter_clickable_image_widget import ClickableImageWidget\n",
    "\n",
    "\n",
    "# Camera and Motor Interface for JetBot\n",
    "from jetbot import Robot, Camera, bgr8_to_jpeg\n",
    "\n",
    "# Python basic packages for image annotation\n",
    "from uuid import uuid1\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import datetime\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "os.system('systemctl restart nvargus-daemon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robot Control Functions\n",
    "\n",
    "To start the data collection process, we first need to control the movement of the robot and drive it around the roads we want it to follow. These functions are an easy way to control the robot. They utilize parameters for speed and sleep duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Robot\n",
    "import time\n",
    "\n",
    "# Robot object\n",
    "robot = Robot()\n",
    "\n",
    "# Our robot motion control functions, they use speed, direction, and duration.\n",
    "def robotLeft():\n",
    "    robot.left(speed=0.5)\n",
    "    time.sleep(0.3)\n",
    "    robot.stop()\n",
    "\n",
    "def robotRight():\n",
    "    robot.right(speed=0.5)\n",
    "    time.sleep(0.3)\n",
    "    robot.stop()\n",
    "    \n",
    "def robotForward():\n",
    "    robot.forward(speed=0.5)\n",
    "    time.sleep(0.3)\n",
    "    robot.stop()\n",
    "\n",
    "def robotBackward():\n",
    "    robot.backward(speed=0.5)\n",
    "    time.sleep(0.3)\n",
    "    robot.stop()\n",
    "\n",
    "# Sometimes the robot can start moving when these functions are defined if the functions were executed previously. This command is here to ensure that the robot behaves.\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "\n",
    "There is a lot going on in the next block of code, so let's break it down.\n",
    "We need to create a folder to hold the data we collect.\n",
    "That data will be photos and unlike pervious data collection the filenames for each photo is now important. This is because each photo will have two elements: \n",
    "1. An image of the track/road/path\n",
    "2. The best path to take in the current situation, represented by x and y coordiantes.\n",
    "\n",
    "This means the the file names will ahve the following structure:\n",
    "``xy_<x value>_<y value>_<uuid>.jpg``\n",
    "\n",
    "This structure will be used when training the machine learning model.\n",
    "\n",
    "When we train, we load the images and parse the x, y values from the filename.\n",
    "\n",
    "Creating and Using Widgets: widgets are incredibly useful. To use them you first create them, then attach call back functions, and finally display the widget for the user.\n",
    "\n",
    "In this case we use widgets for data collection and control of the Jetbot\n",
    "\n",
    "Instructions:\n",
    "1. Drive your Jetbot around the path.\n",
    "2. At various positions, both on the best possible path and pointing incorrectly towards the edge of the path, click within the image on a path you think the robot should take given the current position.\n",
    "\n",
    "Note: take a wide varitey of data points, not just the best case but also cases in which the Jetbot has turned too much or too little and is in dager of hitting a wall or going outside of the path/road."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories not created because they already exist\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca099abf17c4b02832a06d73cbf815b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(ClickableImageWidget(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d815c6cea76f484a89d64a8773d106ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Button(button_style='success', description='LEFT', layout=Layout(height='64px', â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATASET_DIR = 'dataset_xy'\n",
    "\n",
    "# we have this \"try/except\" statement because these next functions can throw an error if the directories exist already\n",
    "try:\n",
    "    os.makedirs(DATASET_DIR)\n",
    "except FileExistsError:\n",
    "    print('Directories not created because they already exist')\n",
    "\n",
    "# Setup the camera and create a widget to view what the Jetbot sees.\n",
    "camera = Camera()\n",
    "camera_widget = ClickableImageWidget(width=camera.width, height=camera.height)\n",
    "snapshot_widget = ipywidgets.Image(width=camera.width, height=camera.height)\n",
    "traitlets.dlink((camera, 'value'), (camera_widget, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "# Widget Creation: sliders and control buttons\n",
    "button_layout = widgets.Layout(width='128px', height='64px')\n",
    "leftButton = widgets.Button(description='LEFT', button_style='success', layout=button_layout)\n",
    "rightButton = widgets.Button(description='RIGHT', button_style='success', layout=button_layout)\n",
    "forwardButton = widgets.Button(description='FORWARD', button_style='success', layout=button_layout)\n",
    "backwardButton = widgets.Button(description='BACKWARD', button_style='success',layout=button_layout)\n",
    "\n",
    "# Attach callbacks for Jetbot control, these reference to the functions we created earlier\n",
    "leftButton.on_click(lambda x: robotLeft())\n",
    "rightButton.on_click(lambda x: robotRight())\n",
    "forwardButton.on_click(lambda x: robotForward())\n",
    "backwardButton.on_click(lambda x: robotBackward())\n",
    "\n",
    "# A callback for the count textbox widget\n",
    "count_widget = widgets.IntText(description='count', value=len(glob.glob(os.path.join(DATASET_DIR, '*.jpg'))))\n",
    "\n",
    "# Creating two new functions used for collecting photos and naming them.\n",
    "def xy_uuid(x, y):\n",
    "    return 'xy_%03d_%03d_%s' % (x * 50 + 50, y * 50 + 50, uuid1())\n",
    "\n",
    "def save_snapshot(_, content, msg):\n",
    "    if content['event'] == 'click':\n",
    "        data = content['eventData']\n",
    "        x = data['offsetX']\n",
    "        y = data['offsetY']\n",
    "        \n",
    "        # save to disk\n",
    "        #dataset.save_entry(category_widget.value, camera.value, x, y)\n",
    "        uuid = 'xy_%03d_%03d_%s' % (x, y, uuid1())\n",
    "        image_path = os.path.join(DATASET_DIR, uuid + '.jpg')\n",
    "        with open(image_path, 'wb') as f:\n",
    "            f.write(camera_widget.value)\n",
    "        \n",
    "        # display saved snapshot\n",
    "        snapshot = camera.value.copy()\n",
    "        snapshot = cv2.circle(snapshot, (x, y), 8, (0, 255, 0), 3)\n",
    "        snapshot_widget.value = bgr8_to_jpeg(snapshot)\n",
    "        count_widget.value = len(glob.glob(os.path.join(DATASET_DIR, '*.jpg')))\n",
    "        \n",
    "camera_widget.on_msg(save_snapshot)\n",
    "\n",
    "# Setting up the widgets for display and control\n",
    "data_collection_widget = ipywidgets.VBox([\n",
    "    ipywidgets.HBox([camera_widget, snapshot_widget]),\n",
    "    count_widget\n",
    "])\n",
    "\n",
    "jetbot_control_widget = ipywidgets.VBox([ipywidgets.HBox([leftButton, forwardButton, backwardButton ,rightButton])\n",
    "])\n",
    "\n",
    "# Now that we've created the widgets and attached callback functions to each of them\n",
    "# we can display the widgets and create the user interface for our Jetbot\n",
    "display(data_collection_widget)\n",
    "\n",
    "display(jetbot_control_widget)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}